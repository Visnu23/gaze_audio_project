# gaze_audio_project
A software-based gaze detection system that automatically switches audio output to the screen youâ€™re looking at â€” no extra hardware.

Infostrom â€” Gaze-Driven Selective Audio Project (IBM LinuxOne)
Overview

Infostrom is an interactive Jupyter notebook demo that simulates gaze-driven audio focus using simple face-position logic.
This version is specifically designed for online Jupyter environments where direct webcam access may be limited or unavailable.

The demo illustrates how gaze estimation can be used to selectively activate or focus audio streams, mimicking how a system might dynamically adjust audio based on where a user is looking.

ğŸ¯ Purpose

To provide an interactive simulation of a gaze-controlled audio environment, demonstrating:

Face-positionâ€“based "gaze" detection.

Zone-based focus logic across multiple monitors.

Real-time visual feedback of active focus and audio simulation.

ğŸ§© Features

âœ… Inline frame display â€” Runs entirely inside a Jupyter Notebook (no external GUI windows required).

ğŸ–¥ï¸ 8 virtual monitors â€” The display is divided into eight logical zones, representing different screens or audio sources.

ğŸ‘ï¸ Simple face-position gaze estimation â€” The position of a detected face determines which monitor zone is currently â€œlooked at.â€

ğŸ”Š Visual and simulated audio feedback â€” The active monitor is highlighted, and its simulated audio status (on/off) is displayed.

âš™ï¸ Requirements

Python 3.x

Jupyter Notebook or JupyterLab

OpenCV

NumPy

(Optional) Audio simulation or placeholder logic

ğŸš€ Usage

Open the notebook in Jupyter.

Run all cells to initialize the environment.

If webcam access is restricted, use simulated face positions or static image input.

Observe the visual indicators showing which â€œmonitorâ€ is currently active and the corresponding simulated audio focus.

ğŸ§  Conceptual Flow

Face Detection â†’ Detect or simulate face coordinates.

Zone Mapping â†’ Determine which of the 8 virtual monitors the face is pointing toward.

Focus Simulation â†’ Visually and logically activate the corresponding audio source.

ğŸ“¦ Notes

This version avoids the use of external windows or GUI dependencies to ensure compatibility with cloud-based and sandboxed environments (e.g., IBM LinuxOne, JupyterHub, or Google Colab).

Future versions may integrate with real gaze-tracking APIs and spatial audio systems.

ğŸ§‘â€ğŸ’» Authors & Acknowledgements

Developed as part of the IBM Z Datathon.
Inspired by gaze-driven interfaces and selective attention models in human-computer interaction.
